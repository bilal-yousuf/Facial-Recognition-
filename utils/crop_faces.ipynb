{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Do not need to run this file\n",
    "\n",
    "This file was used to crop the train and test images to just the faces. The output of it can be found in the test_cropped and train_cropped folders. If you choose to run it, please make sure to delete the content of JUST the test_cropped and train_cropped folders."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2\n",
    "import sys\n",
    "\n",
    "# grab the original images\n",
    "def clean_filename(path):\n",
    "    strings = path.split('/')\n",
    "    return strings[-1]\n",
    "\n",
    "train_images = {}\n",
    "test_images = {}\n",
    "\n",
    "for file in glob.glob(\"pictures/train/*.jpg\"):\n",
    "    train_images[clean_filename(file)] = cv2.cvtColor(cv2.imread(file), cv2.COLOR_BGR2RGB)\n",
    "for file in glob.glob(\"pictures/test/*.jpg\"):\n",
    "    test_images[clean_filename(file)] = cv2.cvtColor(cv2.imread(file), cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    \n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_alt.xml')\n",
    "def save_cropped_faces(face_coordinates, images):\n",
    "    cropped_images = []\n",
    "    \n",
    "    # store minimum sized face detected\n",
    "    # scale all faces to be of the same size\n",
    "    # this allows us to train our classifier without dimensional errors\n",
    "    min_size = sys.maxsize\n",
    "    min_shape = (-1,-1)\n",
    "    for key in images:\n",
    "        # Detect faces\n",
    "        if key in face_coordinates.keys():\n",
    "            x, y, w, h = face_coordinates[key]  \n",
    "        else:\n",
    "            faces = face_cascade.detectMultiScale(images[key], 1.05, 5,flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "            x, y, w, h = faces[0]\n",
    "            \n",
    "        face = images[key][y : y+h, x : x+w]\n",
    "        face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
    "        cropped_images.append((key, face))\n",
    "                \n",
    "        num_pixels = face.shape[0] * face.shape[1]\n",
    "        if num_pixels < min_size:\n",
    "            min_size = num_pixels\n",
    "            min_shape = face.shape\n",
    "                \n",
    "    for img in cropped_images:\n",
    "        resized = cv2.resize(img[1], min_shape[:2])\n",
    "        if \"test\" in img[0]:\n",
    "            cv2.imwrite(\"./pictures/test_cropped/\" + str(img[0]), resized)\n",
    "        else:\n",
    "            cv2.imwrite(\"./pictures/train_cropped/\" + str(img[0]), resized)\n",
    "    \n",
    "face_coords = {\n",
    "    \"train43.jpg\": (100, 80, 30, 30),\n",
    "    \"train31.jpg\": (105, 85, 30, 30),\n",
    "    \"train36.jpg\": (105 ,70, 50, 50),\n",
    "    \"train29.jpg\": (105, 100, 30, 30),\n",
    "    \"train56.jpg\": (110, 65, 30, 30),\n",
    "    \"train21.jpg\": (110, 65, 40, 40),\n",
    "    \"train72.jpg\": (105, 90, 35, 35),\n",
    "    \"train45.jpg\": (100, 90, 35, 35),\n",
    "    \"train9.jpg\": (100, 50, 60, 60),\n",
    "}\n",
    "\n",
    "test_face_coords = {\n",
    "    \"test0.jpg\": (90,50,60,60),\n",
    "    \"test1.jpg\": (105,50,50,50),\n",
    "    \"test2.jpg\": (90,55,50,50),\n",
    "    \"test3.jpg\":(85,65,40,40),\n",
    "    \"test4.jpg\": (78,65,45,45),\n",
    "    \"test5.jpg\": (105,65,45,45),\n",
    "    \"test6.jpg\": (110,65,60,60),\n",
    "    \"test7.jpg\": (110,85,50,50),\n",
    "    \"test8.jpg\": (110,70,50,50),\n",
    "    \"test9.jpg\": (110,70,40,40),\n",
    "    \"test10.jpg\": (90,40,60,60),\n",
    "    \"test11.jpg\": (110,80,45,45),\n",
    "    \"test12.jpg\": (100,50,60,60),\n",
    "    \"test13.jpg\": (90,50,50,50),\n",
    "    \"test14.jpg\": (80,70,50,50),\n",
    "    \"test16.jpg\": (90,50,70,70),\n",
    "    \"test21.jpg\": (110,30,70,70),\n",
    "    \"test24.jpg\": (90,80,40,40),\n",
    "    \"test30.jpg\": (110,45,40,40),\n",
    "    \"test31.jpg\": (100,60,50,50),\n",
    "    \"test33.jpg\": (80,80,50,50),\n",
    "    \"test38.jpg\": (80,80,40,40),\n",
    "    \"test39.jpg\": (110,60,60,60),\n",
    "    \"test41.jpg\": (90,50,50,50),\n",
    "    \"test44.jpg\": (120,50,40,40),\n",
    "    \"test46.jpg\": (110,40,70,70),\n",
    "    \"test47.jpg\": (80,40,60,60),\n",
    "    \"test50.jpg\": (100,50,60,60),\n",
    "    \"test52.jpg\": (100,100,40,40),\n",
    "    \"test55.jpg\": (100,50,50,50),\n",
    "    \"test56.jpg\": (90,50,50,50),\n",
    "    \"test57.jpg\": (110,60,50,50),\n",
    "    \"test58.jpg\": (80,70,50,50),\n",
    "    \"test59.jpg\": (120,40,60,60),\n",
    "    \"test62.jpg\": (100,60,50,50),\n",
    "    \"test65.jpg\": (100,50,45,45),\n",
    "    \"test67.jpg\": (120,80,45,45)\n",
    "}\n",
    "\n",
    "train_images.update(test_images)\n",
    "face_coords.update(test_face_coords)\n",
    "\n",
    "save_cropped_faces(face_coords, train_images)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}